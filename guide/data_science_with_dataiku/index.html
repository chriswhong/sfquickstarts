
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Accelerating Data Science with Snowflake and Dataiku</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="data_science_with_dataiku"
                  title="Accelerating Data Science with Snowflake and Dataiku"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="Lab Overview" duration="1">
        <p>This Snowflake Quickstart introduces you to the basics of using Snowflake together with Dataiku Cloud as part of a Data Science project. We&#39;ll be highlighting some of the well-integrated functionalities between the two cloud-based technologies. It is designed specifically for use with the <a href="https://trial.snowflake.com" target="_blank">Snowflake free 30-day trial</a>, and the Dataiku Cloud trial version via Snowflake&#39;s Partner Connect. In addition, we&#39;ll also take a look at Snowflake&#39;s Data Marketplace and how 3rd party data sets from there can enrich and support your Data Science projects.</p>
<p>This Snowflake lab will be done as part of a theoretical real-world &#34;story&#34; with COVID-19 data to help you better understand why we are performing the steps in this lab and in the order they appear.</p>
<p>The story of this lab is examining incidents of COVID-19 in Europe along with changes in mobility. We would like to use historical data to predict mortality rates for COVID-19 and see if changes in mobility is a significant factor driving this.</p>
<p>The source of data is the Starschema Covid-19 database from the Snowflake Data Marketplace. After analyzing the data in Snowflake&#39;s Snowsight, we will move over to Dataiku to perform some transformations (that are pushed down to Snowflake for computation) and then create a predictive model.</p>
<p>The specific tables we&#39;ll use are the JHU_COVID_19 dataset which reports case numbers by province and day as well as the GOOG_GLOBAL_MOBILITY_REPORT which reports the percent change in mobility across various areas (e.g., grocery stores, parks, etc.) by a state/region of a country and day.</p>
<p>Finally, we will write those predictions back into Snowflake.</p>
<p>Upon completion of the lab you should be able to grasp how to work a simple Data Science project using Snowflake and Dataiku.</p>
<h2 is-upgraded>Prerequisites</h2>
<ul>
<li>Use of the Snowflake free 30-day trial environment</li>
<li>Basic knowledge of SQL, and database concepts and objects</li>
</ul>
<h2 class="checklist" is-upgraded>What You&#39;ll Learn</h2>
<p>The exercises in this lab will walk you through the steps to:</p>
<ul>
<li>Create databases, tables, views, and warehouses in Snowflake</li>
<li>Access data in Snowflake using the Data Marketplace</li>
<li>Use Snowflake&#39;s &#34;Partner Connect&#34; to seamlessly create a Dataiku DSS Cloud trial</li>
<li>Create a Data Science project in Dataiku and perform analysis on data via Dataiku within Snowflake</li>
<li>Create, run, and evaluate simple Machine Learning models in Dataiku</li>
<li>Write results back to Snowflake</li>
<li>Use cloning and time travel for test environment</li>
</ul>
<h2 is-upgraded>What You&#39;ll Build</h2>
<ul>
<li>A Snowflake and Dataiku integration to create, run, and evaluate COVID-19 machine learning models.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Prepare Your Lab Environment" duration="5">
        <p>If you haven&#39;t already, register for a  <a href="https://trial.snowflake.com?utm_source=Snowflake&utm_medium=labguide&utm_campaign=dataiku-vhol-download-free-trial-from-lab-guide" target="_blank">Snowflake free 30-day trial</a>.</p>
<p>Please select a region which is physically closest to you, and select the Enterprise edition so you can leverage some advanced capabilities that are not available in the Standard Edition.</p>
<p>After registering, you will receive an email with an activation link and your Snowflake account URL. Bookmark this URL for easy, future access. After activation, you will create a user name and password. Write down these credentials.</p>
<p>Resize your browser window, so that you can view this guide and your web browser side-by-side and follow the lab instructions. If possible, use a secondary display dedicated to the lab guide.</p>
<h3 is-upgraded>Download lab resources:</h3>
<p><a href="https://snowflake-corp-se-workshop.s3-us-west-1.amazonaws.com/VHOL_Snowflake_Dataiku/Snowflake_Dataiku_VHOL.sql" target="_blank"><paper-button class="colored" raised><iron-icon icon="file-download"></iron-icon>Download sample SQL</paper-button></a></p>
<aside class="warning"><p><strong>About the screen captures, sample code, and environment <br></strong> Screen captures in this lab depict examples and results that may slightly vary from what you may see when you complete the exercises.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="The Snowflake User Interface" duration="10">
        <h2 is-upgraded>Logging Into the Snowflake User Interface (UI)</h2>
<p>Open a browser window and enter the URL of your Snowflake 30-day trial environment. You should see the login screen below. Enter your unique credentials to log in.</p>
<p class="image-container"><img alt="img" src="img/6d654a9d75da11a5.png"></p>
<h2 is-upgraded>Close any Welcome Boxes and Tutorials</h2>
<p>You may see &#34;welcome&#34; and &#34;helper&#34; boxes in the UI when you log in for the first time. Also a &#34;Enjoy your free trial...&#34; ribbon at the top of the UI. Minimize and close them by clicking on the items in the red boxes in the screenshot below.</p>
<p class="image-container"><img alt="img" src="img/d17735009c1780b0.png"></p>
<h2 is-upgraded>Navigating the Snowflake UI</h2>
<p>First let&#39;s get you acquainted with Snowflake! This section covers the basic components of the user interface to help you orient yourself. We will move left to right in the top of the UI.</p>
<p>The top menu allows you to switch between the different areas of Snowflake:</p>
<p class="image-container"><img alt="img" src="img/a9f7d966f0a68ad9.png"></p>
<p>The <strong>Databases</strong> tab shows information about the databases you have created or have privileges to access. You can create, clone, drop, or transfer ownership of databases as well as load data (limited) in the UI. Notice several databases already exist in your environment. However, we will not be using these in this lab.</p>
<p class="image-container"><img alt="img" src="img/2d9765a3f5e3f00.png"></p>
<p>The <strong>Shares</strong> tab is where data sharing can be configured to easily and securely share Snowflake table(s) among separate Snowflake accounts or external users, without having to create a second copy of the table data.</p>
<p>The <strong>Warehouses</strong> tab is where you set up and manage compute resources (virtual warehouses) to load or query data in Snowflake. Note a warehouse called &#34;COMPUTE_WH (XS)&#34; already exists in your environment.</p>
<p class="image-container"><img alt="img" src="img/b7f1451403deaa78.png"></p>
<p>The <strong>Worksheets</strong> tab provides an interface for submitting SQL queries, performing DDL and DML operations and viewing results as your queries/operations complete. The default &#34;Worksheet 1&#34; appears.</p>
<p>In the left pane is the database objects browser which enables users to explore all databases, schemas, tables, and views accessible by the role selected for a worksheet. The bottom pane shows results of queries and operations.</p>
<p>The various windows on this page can be resized by moving the small sliders on them. And if during the lab you need more room to work in the worksheet, collapse the database objects browser in the left pane. Many of the screenshots in this guide will have this database objects browser closed.</p>
<p class="image-container"><img alt="img" src="img/b185d55659d1b956.png"></p>
<p>At the top left of the default &#34;Worksheet 1,&#34; just to the right of the worksheet tab, click on the small, downward facing arrow, select &#34;Load Script&#34;, then browse to the &#34;lab_scripts.sql&#34; file you downloaded in the prior module and select &#34;Open&#34;. All of the SQL commands you need to run for the remainder of this lab will now appear on a new worksheet.</p>
<p><strong>Do not run any of the SQL commands yet. We will come back to them later in the lab and execute them one at a time</strong></p>
<p class="image-container"><img alt="img" src="img/7d6e1ee9826af584.png"></p>
<p>Rename the newly created worksheet to Covid19 by clicking on the worksheet name and typing Covid19 and pressing ‘Enter&#39;</p>
<p class="image-container"><img alt="img" src="img/c101a8a7254155a3.png"></p>
<p class="image-container"><img alt="img" src="img/ce2cef5515bfaf31.png"></p>
<aside class="warning"><p><strong>Warning - Do Not Copy/Paste SQL to a Worksheet <br></strong> Copy-pasting the SQL code into a Snowflake worksheet may result in formatting errors and the SQL may not run correctly. Make sure to use the &#34;Load Script&#34; method just covered.  On older or locked-down browsers, this &#34;load script&#34; step may not work as the browser will prevent you from opening the .sql file. If this is the case, open the .sql file with a text editor and then copy/paste all the text from the .sql file to the &#34;Worksheet 1&#34;</p>
</aside>
<aside class="special"><p><strong>Worksheets vs the UI <br></strong> Much of the configurations in this lab will be executed via this pre-written SQL in the Worksheet in order to save time. These configurations could also be done via the UI in a less technical manner but would take more time.</p>
</aside>
<p>The <strong>History</strong> tab allows you to view the details of all queries executed in the last 14 days in the Snowflake account (click on a Query ID to drill into the query for more detail).</p>
<p class="image-container"><img alt="img" src="img/2b27dc066a0eea3e.png"></p>
<p>If you click on the top right of the UI where your username appears, you will see that you can change your password, roles, or preferences. Snowflake has several system defined roles. You are currently in the default role of SYSADMIN. We will change this in the next part of the lab.</p>
<p class="image-container"><img alt="img" src="img/796a6314a0364279.png"></p>
<aside class="warning"><p><strong>SYSADMIN <br></strong> For most of this lab you will remain in the SYSADMIN (aka System Administrator) role which has privileges to create warehouses and databases and other objects in an account. In a real-world environment, you would use different roles for the tasks in this lab, and assign the roles to your users. More on access control in Snowflake is in towards the end of this lab and also in our <a href="https://docs.snowflake.net/manuals/user-guide/security-access-control.html" target="_blank">documentation</a></p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Prepare Dataiku Trial Account via Snowflake Partner Connect" duration="10">
        <h2 is-upgraded>Create Dataiku trial via Partner Connect</h2>
<p>At the top right of the page, confirm that your current role is <code>ACCOUNTADMIN</code>, by clicking on your profile on the top right.</p>
<p class="image-container"><img alt="img" src="img/efb8b397cf3e9ad9.png"></p>
<p>Click on Partner Connect at the top bar and scroll down to find Dataiku.</p>
<p class="image-container"><img alt="img" src="img/2b21c712290db24f.png"></p>
<p>Click on the Dataiku tile. This will launch the following window, which will automatically create the connection parameters required for Dataiku to connect to Snowflake. Snowflake will create a dedicated database, warehouse, system user, system password and system role, with the intention of those being used by the Dataiku account.</p>
<pre><code>Database:        PC_DATAIKU_DB
Warehouse:       PC_DATAIKU_WH (X-Small)
System User:     PC_DATAIKU_USER
System Password: Autogenerated &amp; Randomized
System Role:     PC_DATAIKU_ROLE
                 Role PUBLIC will be granted to the PC_DATAIKU_RLE
                 Role PC_DATAIKU_ROLE will be granted to the SYSADMIN role

Database(s) with USAGE privilege granted: leave this blank

</code></pre>
<p>We&#39;d like to use the <strong>PC_DATAIKU_USER</strong> to connect from Dataiku to Snowflake, and use the <strong>PC_DATAIKU_WH</strong> when performing activities within Dataiku that are pushed down into Snowflake.</p>
<p>This is to show that a Data Science team working on Dataiku and by extension on Snowflake can work completely independently from the Data Engineering team that works on loading data into Snowflake using different roles and warehouses.</p>
<p>Note that the user password (which is autogenerated by Snowflake and never displayed), along with all of the other Snowflake connection parameters, are passed to the Dataiku server so that they will automatically be used for the Dataiku connection. <strong>DO NOT CHANGE THE PC_DATAIKU_USER password</strong>, otherwise Dataiku will not be able to connect to the Snowflake database.</p>
<p class="image-container"><img alt="img" src="img/6390df764914952c.png"></p>
<aside class="warning"><p><strong>Informational Note: <br></strong> If you are using a different Snowflake account than the one created at the start, you may get the following screens asking for your email details:</p>
</aside>
<p class="image-container"><img alt="img" src="img/a27ad9ca5fd32938.png"></p>
<aside class="warning"><p>Click on ‘Go to Preferences&#39; and populate with your email details</p>
</aside>
<p class="image-container"><img alt="img" src="img/6620c704aad23207.png"></p>
<p class="image-container"><img alt="img" src="img/7705e2fbbe95c91d.png"></p>
<p>Click on <strong>Connect</strong>. You may be asked to provide your first and last name. If so, add them and click Connect.</p>
<p class="image-container"><img alt="img" src="img/cb1cb1489e65e3b6.png"></p>
<p>Your partner account has been created. Click on <strong>Activate</strong> to get it activated.</p>
<p class="image-container"><img alt="img" src="img/37e8ecd5775a856e.png"></p>
<p>This will launch a new page that will redirect you to a launch page from Dataiku.</p>
<p>Here, you will have two options:</p>
<ol type="1">
<li>Login with an existing Dataiku username</li>
<li>Sign up for a new Dataiku account</li>
</ol>
<p>We assume that you&#39;re new to Dataiku, so ensure the &#34;Sign Up&#34; box is selected, and sign up with either GitHub, Google or your email address and your new password. Click sign up.</p>
<p class="image-container"><img alt="img" src="img/2d9a402dcd27724f.png"></p>
<p>When using your email address, ensure your password fits the following criteria:</p>
<ol type="1">
<li>At least 8 characters in length</li>
<li>Should contain: Lower case letters (a-z) Upper case letters (A-Z) Numbers (i.e. 0-9)</li>
</ol>
<p>You should have received an email from Dataiku to the email you have signed up with.</p>
<p class="image-container"><img alt="img" src="img/5b767a0efe3b171c.png"></p>
<p>Activate your Dataiku account via the email sent.</p>
<h2 is-upgraded>Review Dataiku Setup</h2>
<p>Upon clicking on the activation link, please briefly review the Terms of Service of Dataiku Cloud. In order to do so, please scroll down to the bottom of the page. Click on <strong>I AGREE</strong>.</p>
<p class="image-container"><img alt="img" src="img/449287ba394d8a8e.png"></p>
<p class="image-container"><img alt="img" src="img/9c0e19b464204993.png"></p>
<p>Next, you&#39;ll need to complete your sign up with the following information:</p>
<ol type="1">
<li>First Name</li>
<li>Last Name</li>
<li>Job Title</li>
<li>How would you define yourself (drop-down options for roles)</li>
<li>Company Name</li>
<li>Company Size (drop-down)</li>
<li>What is your country (drop-down)</li>
<li>What is your goal with Dataiku Cloud? <em>(optional)</em></li>
</ol>
<p>Then click on <strong>Start</strong>.</p>
<p class="image-container"><img alt="img" src="img/f388f7bcb75ae124.png"></p>
<p class="image-container"><img alt="img" src="img/57930403485c1a0b.png"></p>
<ol type="1">
<li>You will be redirected to the Dataiku Cloud Launchpad site. Click <strong>GOT IT!</strong> to continue.</li>
</ol>
<p class="image-container"><img alt="img" src="img/37bb275bd8d7149e.png"></p>
<p class="image-container"><img alt="img" src="img/4e14ec316291317.png"></p>
<p>You&#39;ve now successfully set up your Dataiku trial account via Snowflake&#39;s Partner Connect. We are now ready to continue with the lab. For this, move back to your Snowflake browser.</p>
<aside class="warning"><p>Remember that the user password (which is autogenerated by Snowflake and never displayed), along with all of the other Snowflake connection parameters, are passed to the Dataiku server so that they will automatically be used for the Dataiku connection. DO NOT CHANGE THE PC_DATAIKU_USER password, otherwise Dataiku will not be able to connect to the Snowflake database.</p>
</aside>
<p class="image-container"><img alt="img" src="img/ea884c1ea55e4d2b.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Getting and Preparing COVID-19 Data from the Snowflake Data Marketplace" duration="20">
        <aside class="special"><p><strong>Snowflake&#39;s Data Sharing <br></strong> The Snowflake Data Marketplace is Snowflake&#39;s hosted data exchange, where any Snowflake customer can seamlessly access third-party data, and where companies can publish data and offer data services to be discovered and purchased by other Snowflake customers. Today it contains data from over 100+ Data Providers with over 220+ data listings. Review <a href="https://www.snowflake.com/data-marketplace/" target="_blank">this webpage</a> for more information. <br> You can also build out your own Snowflake Data Exchange and remove the need for FTP, file shares, ETL processes to create the files for your customers and standardize on Snowflake Data Exchange. This will break down data silos and allows you to frictionlessly share data in a highly governed way within your company, as well as with suppliers, partners or customers. Your Data Exchange governs live access to a single copy of the data, allows you to securely control access to data, and eliminates the cost and headaches of traditional data sharing methods like copying files or building ETL pipelines.</p>
</aside>
<h2 is-upgraded>Getting COVID-19 Data from the Snowflake Data Marketplace</h2>
<p>To access the Snowflake Data Marketplace, follow the below steps.</p>
<p>Make sure that your current role is <code>ACCOUNTADMIN</code> (as shown in the top right). If not, update your role by clicking on your profile on the top right.</p>
<p class="image-container"><img alt="img" src="img/b66702a62bde2f12.png"></p>
<p>Select the <strong>Data Marketplace</strong> icon at the top and click on <strong>Explore the Snowflake Data Marketplace</strong>.</p>
<p class="image-container"><img alt="img" src="img/cc39eb881a58aa42.png"></p>
<p>If it&#39;s your first time using the Data Marketplace, the following screens will appear. Click <strong>Sign in to continue</strong> and enter your login information in the next step.</p>
<p class="image-container"><img alt="img" src="img/277d5f00159ff716.png"></p>
<p>You will now be redirected to the Snowflake Data Marketplace.</p>
<p>It will display the variety of external data sources across a variety of industries. The Search box in the top right allows you to search for a listing or data provider. The side menu on the left shows the categories of data available in the Data Marketplace.</p>
<p>Since we&#39;ll be looking at COVID-19 data, select the <strong>Health</strong> Category on the left-hand-side by clicking on it.</p>
<p class="image-container"><img alt="img" src="img/f41de211721ccfad.png"></p>
<p>A number of COVID-19 data sets are shown here - we will be using <strong>Starschema&#39;s COVID-19 Epidemiological Data</strong>. Click on that tile.</p>
<p class="image-container"><img alt="img" src="img/518908890ae528e5.png"></p>
<p>The Starschema dataset provides several tables, with daily COVID-19 incident data from the John Hopkins University, as well as local health authorities such as the European Centre for Disease Prevention and Control (ECDC), the German Robert-Koch-Institut (RKI) and American authorities. In addition, demographic data, mobility data and information about global government measures is made available in this dataset.</p>
<p>Ensure you&#39;re in the <code>ACCOUNTADMIN</code> role on the top right; if not, click on the person icon to change it. Then click <strong>Get Data</strong>.</p>
<p class="image-container"><img alt="img" src="img/7fe900653bee2065.png"></p>
<p>Change the Database name to <strong>COVID19</strong>. In addition, select all the roles to <strong>Public</strong>, so that you can access the database (click off the selection box once you have selected public) and <strong>Accept</strong> Snowflake&#39;s consumers terms and Starschema&#39;s terms of use, and finally click on <strong>Create Database</strong>.</p>
<p class="image-container"><img alt="img" src="img/9756b0ed7cd1182b.png"></p>
<p>Click <strong>View Database</strong> for a quick peek into the database and its schemas. You&#39;ll see two schemas: INFORMATION_SCHEMA and PUBIC. Click on <strong>PUBLIC</strong> to see the available tables:</p>
<p class="image-container"><img alt="img" src="img/2682c44c90abe261.png"></p>
<p class="image-container"><img alt="img" src="img/e0e98261110e75e9.png"></p>
<p>If you enter JHU in the top right hand filter you will see all tables with data sourced from <a href="https://www.jhu.edu/" target="_blank">John Hopkins University</a></p>
<p class="image-container"><img alt="img" src="img/20171ef97cf3e211.png"></p>
<p>You will also see that All Types has been selected and you are using the <code>ACCOUNTADMIN</code> role by default.</p>
<p>Further to the right you will see the Database Details including: the owner of the share, number of tables and views in this database, source, share name and which data provider shared the information with you.</p>
<p class="image-container"><img alt="img" src="img/8150080e765bce1.png"></p>
<p>You have now successfully subscribed to the COVID-19 dataset from StarSchema which is updated daily with global COVID data. Note that we didn&#39;t have to create databases, tables, views, or ETL processes. We simply can search for and access shared data from the Snowflake Data Marketplace.</p>
<h2 is-upgraded>Analysing COVID-19 using Snowsight</h2>
<p>Now that we&#39;ve done some preparation work, let&#39;s do some primarily data analysis on our COVID-19 data. For this we will use Snowsight, the SQL Worksheets replacement, which is designed to support data analyst activities.</p>
<p>Snowflake recently released the next generation of it&#39;s analytics UI — <strong>Snowsight</strong>. On top of a redesigned interface, there are many improvements for analysts, data engineers, and business users. With Snowsight, it is easier and faster to write queries and get results and collaboration with others through sharing makes it easier to explore and visualize data across your organization. Snowsight includes many features and enhancements, including:</p>
<ul>
<li><strong>Fast query writing:</strong> Includes smart autocomplete for query syntax keywords or listing values that match table/column names, data filters and quick access to Snowflake documentation for specific functions.</li>
<li><strong>Interactive query results:</strong> View summary statistics about the data that has been returned by their query, using histograms of the distribution to identify outliers and anomalies.</li>
<li><strong>Attractive data visualizations:</strong> Quickly analyze data without requiring an external analytics/visualization tool, with automatic chart generation and drag-and-drop interface for creating dashboards.</li>
<li><strong>Sharing and collaboration:</strong> Share queries, worksheets, visualizations and dashboards securely among teams.</li>
<li><strong>Schema browser:</strong> Search instantly across databases and schemas accessible by the current session role for tables, views, and columns whose names contain a specified string. Pin tables for quick reference to see column names and data types.</li>
</ul>
<p>For more information on using Snowsight, see the <a href="https://docs.snowflake.com/en/user-guide/ui-snowsight.html" target="_blank">documentation</a>.</p>
<p>Let&#39;s run some preliminary analysis on the two tables that we&#39;ll focus on. For this, we will select <strong>Worksheets</strong> in the top left corner.</p>
<p class="image-container"><img alt="img" src="img/71de13a48b29e458.png"></p>
<p>If this is the first time you&#39;ve used Snowsight, you will be prompted to enable it.</p>
<p class="image-container"><img alt="img" src="img/977a898b207234a.png"></p>
<p>Click &#34;Enable Worksheets and Dashboards&#34;, and then click on &#34;Import Worksheets&#34;</p>
<p>Open up the script we loaded earlier by clicking on the that <strong>COVID19 Worksheet</strong>.</p>
<p class="image-container"><img alt="img" src="img/5f3bb9318348adc.png"></p>
<p>In the worksheet, ensure the correct context, on the top right: using the SYSADMIN role and the COMPUTE_WH that was created when your trial account was created.</p>
<p class="image-container"><img alt="img" src="img/c2e084f9ff009dbd.png"></p>
<p>In addition, select the correct database <strong>COVID19</strong> on the top left of the worksheet. <img alt="img" src="img/29a1b5288bb95581.png"></p>
<p>We&#39;ll run a simple analysis of confirmed cases over time for a few European countries, incl. Germany, UK, France, Italy, Spain, The Netherlands, Poland and Sweden.</p>
<p>Run the command below as provided in the script. Highlight the following text in the Covid19 worksheet and hit <strong>‘Run&#39;</strong>.</p>
<p class="image-container"><img alt="img" src="img/6d779ea66c4c708.png"></p>
<pre><code language="language-sql" class="language-sql">use role SYSADMIN;
use database COVID19;
use warehouse COMPUTE_WH;

select country_region, sum(cases), case_type, date from public.jhu_covid_19 where case_type=&#39;Confirmed&#39; and country_region in (&#39;Germany&#39;,&#39;United Kingdom&#39;, &#39;France&#39;, &#39;Italy&#39;, &#39;Spain&#39;, &#39;Netherlands&#39;, &#39;Poland&#39;, &#39;Sweden&#39;) group by date, country_region, case_type;

</code></pre>
<p>Your result will include a table of results. Click on <strong>Chart</strong> to get a simple visualization of the results:</p>
<p class="image-container"><img alt="img" src="img/f66028d738556972.png"></p>
<p>On the right hand side of the chart, change the values as follows:</p>
<ul>
<li><strong>Line chart</strong></li>
<li><strong>SUM(CASES)</strong> for the y-axis, to show the sum of confirmed values in a line</li>
<li><strong>DATE</strong> for the x-axis</li>
<li>Add Column, then Country_region</li>
<li><strong>COUNTRY_REGION</strong> as the country for each of which the sum of confirmed cases will be shown</li>
</ul>
<p class="image-container"><img alt="img" src="img/80d49348c1c5fe9c.png"></p>
<ul>
<li><em>Optional: Label the x-axis as &#34;Date&#34; and the y-axis as &#34;Sum of confirmed cases&#34;</em></li>
</ul>
<p class="image-container"><img alt="img" src="img/7e64b8f73924b93d.png"></p>
<p>We clearly see that the number of confirmed COVID-19 cases rapidly increased in March until mid April, after which - due to several lockdown measures in place - the increase slowed down significantly.</p>
<p>However, with the start of the autumn season in September, we clearly see an exponential and worrying rise in COVID-19 confirmed cases, culminating in over or around 1 million confirmed cases for 3 major European countries.</p>
<p>In today&#39;s news, people are typically aware of newly known infections. We can also review just how many people have an active COVID-19 infection (essentially Confirmed cases minus Recovered, which are the ‘Active&#39; case types).</p>
<p>Remain on the chart window, and run the next command, as the execution of this command will result in the corresponding chart:</p>
<pre><code language="language-sql" class="language-sql">select country_region, sum(cases), case_type, date from public.jhu_covid_19 where case_type=&#39;Active&#39; and country_region in (&#39;Germany&#39;,&#39;United Kingdom&#39;, &#39;France&#39;, &#39;Italy&#39;, &#39;Spain&#39;, &#39;Netherlands&#39;, &#39;Poland&#39;, &#39;Sweden&#39;) group by date, country_region, case_type;
</code></pre>
<p class="image-container"><img alt="img" src="img/a8b41c4f8603230f.png"></p>
<p><strong>Note that this refers to the sum of active cases on the y-axis.</strong></p>
<p>From here, we can easily deduce that the majority of countries were able to get infections &#34;under control&#34; after the end of April. During summertime, the active infection cases in most countries were relatively stable or had even decreased significantly. With the second wave underway, Europe is seeing exponential growth in active cases with the rise in confirmed cases.</p>
<h2 is-upgraded>Data Problem</h2>
<p>Sometimes you go through the entire process of building a predictive model and the predictions are quite poor and you trace the issue back to data problems.  In other cases, such as this one, the data changes with time and the models go bad.  After creating the lab we had to return and dig around in the data to see what the problem was.  In summary, we used the same Snowsight visualization capabilities as above to determine that for some reason around June 2 and June 3 2021, there were a massive number of &#34;negative deaths&#34; that were throwing off the models.  The count should only be a positive number.   After some further digging around using Snowsight, it was discovered that Peru reported problematic data that was causing the problems with the model accuracy.<br> As an optional step to discover this data problem, you can run the following SQL statement:</p>
<pre><code language="language-sql" class="language-sql">select DATE, province_state, SUM(CASES) 
from public.jhu_covid_19 
WHERE CASE_TYPE = &#39;Deaths&#39;
and DATE &gt;=&#39;2021-05-25&#39;
and DATE &lt;=&#39;2021-06-06&#39;
and country_region = &#39;Peru&#39;
group by province_state, DATE 
order by province_state, date;
</code></pre>
<p>Then create a visualization like this:</p>
<p class="image-container"><img alt="img" src="img/3ab75dc66cae2ee.png"></p>
<p>You will notice that Peru normally states how many people have died to date from COVID broken out by State but on on June 2nd 2021, most of the deaths were assigned to a new &#34;Unknown&#34; State then reported correctly after that.  Later on when we find the difference in total deaths per State per day, this leads to some massive negative death counts all on two days in Peru.  There are better ways to fix this data problem but an easy for this lab is to simply filter Peru out of the solution.  We will do that in a future step.</p>
<h2 is-upgraded>Preparing the Data for Further Data Analysis and Consumption</h2>
<p>Let&#39;s now create views that reference the shared COVID19 tables we&#39;ll be using. Switch back to the OLD UI and select the <strong>Worksheets</strong> tab.</p>
<p class="image-container"><img alt="img" src="img/ce9ba96a054c6412.png"></p>
<p class="image-container"><img alt="img" src="img/212e40be1c0b2090.png"></p>
<pre><code language="language-sql" class="language-sql">use role PC_DATAIKU_ROLE;
use database PC_DATAIKU_DB;
create or replace view JHU_COVID_19 as select * from COVID19.PUBLIC.JHU_COVID_19;
create or replace view GOOG_GLOBAL_MOBILITY_REPORT as select * from COVID19.PUBLIC.GOOG_GLOBAL_MOBILITY_REPORT;
</code></pre>
<p>Notice that we have not selected a warehouse. Statements that create database objects do not utilize a running warehouse, so it&#39;s OK to leave this unselected for now.</p>
<p>Now let&#39;s navigate to the <strong>Warehouses</strong> tab. Note the &#34;Create...&#34; option at the top is where you can quickly create a new warehouse. You&#39;ll see two virtual warehouses – the COMPUTE_WH that we just used for our analysis, and the PC_DATAIKU_WH that was created when we created our Dataiku account through Partner Connect.</p>
<p>Click on the row of this &#34;PC_DATAIKU_WH&#34; warehouse (not the blue hyperlink that says &#34;PC_DATAIKU_WH&#34;) so the entire row is highlighted. Then click on the &#34;Configure...&#34; text above it to see the configuration detail of the &#34;PC_DATAIKU_WH&#34;. We will use this warehouse to build, train, and deploy our models in Dataiku.</p>
<p class="image-container"><img alt="img" src="img/c550bcfd30477816.png"></p>
<p>Let&#39;s walk through the settings of this warehouse as there&#39;s a lot of functionality here, much of which is unique to Snowflake versus other data warehouses.</p>
<p>NOTE - If you do not have a Snowflake Edition of Enterprise or greater, you will <em>NOT</em> see the &#34;Maximum Clusters&#34; or &#34;Scaling Policy&#34; configurations from the screenshot below. Multi-clustering is not utilized in this lab, but we will still discuss it as it is a key capability of Snowflake.</p>
<p>The &#34;Size&#34; drop-down is where the size of the warehouse is selected. For larger data loading operations or more compute-intensive queries, a larger warehouse will be needed. The t-shirt sizes translate to underlying compute nodes, either AWS EC2, Azure Virtual Machines, or Google Compute Engine Virtual Machines. The larger the t-shirt size, the more compute resources from the cloud provider are allocated to that warehouse. As an example, the 4-XL option allocates 128 nodes. Also, this sizing can be changed up or down on the fly with a simple click.</p>
<p>If you have Snowflake Enterprise Edition or greater you will see the Maximum Clusters section. This is where you can set up a single warehouse to be multi-cluster up to 10 clusters. As an example, if the 4-XL warehouse we just mentioned was assigned a maximum cluster size of 10, it could scale up to be 1280 (128 * 10) AWS EC2 or Azure VM nodes powering that warehouse...and it can do this in seconds! Multi-cluster is ideal for concurrency scenarios, such as many business analysts simultaneously running different queries using the same warehouse. In this scenario, the various queries can be allocated across the multiple clusters to ensure they run fast.</p>
<p>The final sections allow you to automatically suspend the warehouse so it suspends (stops) itself when not in use and no credits are consumed. There is also an option to automatically resume (start) a suspended warehouse so when a new workload is assigned to it, it will automatically start back up. This functionality enables Snowflake&#39;s fair &#34;pay as you use&#34; compute pricing model which enables customers to minimize their data warehouse costs.</p>
<p class="image-container"><img alt="img" src="img/265a80a4642cd05a.png"></p>
<aside class="special"><p><strong>Snowflake Compute vs Other Warehouses <br></strong> Many of the warehouse/compute capabilities we just covered, like being able to create, scale up and out, and auto-suspend/resume warehouses are things that are simple in Snowflake and can be done in seconds. Yet for on-premise data warehouses these capabilities are very difficult (or impossible) to do as they require significant physical hardware, over-provisioning of hardware for workload spikes, significant configuration work, and more challenges. Even other cloud data warehouses cannot scale up and down like Snowflake without significantly more configuration work and time.</p>
</aside>
<aside class="warning"><p><strong>Warning - Watch Your Spend!</strong></p>
</aside>
<p>During or after this lab you should <em>NOT</em> do the following without good reason or you may burn through your $400 of free credits more quickly than desired:</p>
<ul>
<li>Disable auto-suspend. If auto-suspend is disabled, your warehouses will continue to run and consume credits even when not being utilized.</li>
<li>Use a warehouse size that is excessive given the workload. The larger the warehouse, the more credits are consumed.</li>
</ul>
<p>We are going to use this virtual warehouse for our Dataiku work. However, we are first going to slightly increase the size of the warehouse to increase the compute power it contains.</p>
<p>Change the size of this data warehouse from X-Small to Medium. Then click the &#34;Finish&#34; button.</p>
<p class="image-container"><img alt="img" src="img/3c854befb538a9ba.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Creating and Running a Dataiku Project" duration="20">
        <p>For this module, we will login into the Dataiku hosted trial account and create a Dataiku project.</p>
<aside class="special"><p><strong>Completed Sample Project <br></strong> We have added a fully finished example of the <strong>COVID-19</strong> project that we will create in this lab and it&#39;s available to you as soon as you log in (along with another example project predicting <strong>Flight Delays</strong> - see screenshot below).</p>
</aside>
<h2 is-upgraded>Creating a Dataiku Project</h2>
<p>Go back to your Dataiku Cloud instance. Click on <strong>OPEN DATAIKU DSS</strong> to get started.</p>
<p class="image-container"><img alt="img" src="img/b7579878b5bb3c6d.png"></p>
<p>Once you&#39;ve logged in, <strong>click</strong> on <strong>+NEW PROJECT</strong> and <strong>select Blank project</strong> to create a new project.</p>
<p class="image-container"><img alt="img" src="img/7303a484132b196a.png"></p>
<p>Name the project <strong>Covid-19</strong> as we&#39;ll be working with COVID-19 data.</p>
<p>Note that the Project key will be automatically populated when creating a name, but you&#39;re free to change it directly.</p>
<p>Success! You&#39;ve now created the COVID-19 project.</p>
<p class="image-container"><img alt="img" src="img/e3f03b35ca76992a.png"></p>
<p>Click on <strong>Got it!</strong> to minimize the pop-up on <strong>Navigation and help in DSS</strong> and return to the project home screen.</p>
<p>Review the Dataiku DSS page. There are a few things to note from the project landing page on an example project:</p>
<ul>
<li>The project name, image associated with the project, collaborators, and optional tags:</li>
</ul>
<p class="image-container"><img alt="img" src="img/d7bb256dbf86e619.jpg"></p>
<ul>
<li>The number and types of objects in the project.</li>
</ul>
<p class="image-container"><img alt="img" src="img/d34f1b983a7d764d.jpg"></p>
<ul>
<li>A description of the project written in markdown, can link specific Dataiku objects (e.g., datasets, saved models, etc.) in the description:</li>
</ul>
<p class="image-container"><img alt="img" src="img/d2b2cadc1dd566ba.jpg"></p>
<ul>
<li>Summary of project (history is saved in a git log) as well as a Chat function for better collaboration:</li>
</ul>
<p class="image-container"><img alt="img" src="img/721c14906c122cfc.jpg"></p>
<h2 is-upgraded>Import Datasets</h2>
<p>Import <strong>JHU_COVID_19</strong> and <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> datasets</p>
<p>Click on <strong>+IMPORT YOUR FIRST DATASET</strong></p>
<p class="image-container"><img alt="img" src="img/99d6f2444622f16d.png"></p>
<p>Under SQL, select <strong>Snowflake</strong></p>
<p class="image-container"><img alt="img" src="img/6cc72a4998e9d90d.png"></p>
<p>To load the JHU table, type in <strong>JHU_COVID_19</strong> (ALL UPPERCASE) under Table and <strong>PUBLIC</strong> under Schema. Then click <strong>TEST TABLE</strong> to test the connection:</p>
<p class="image-container"><img alt="img" src="img/6b81098daa01f76e.jpg"></p>
<p>If successful <strong>click CREATE</strong> to create the table:</p>
<p class="image-container"><img alt="img" src="img/39861ced185265e4.jpg"></p>
<p>Return to the flow by clicking on the <strong>flow</strong> icon in the top left <em>(keyboard shortcut G+F)</em>:</p>
<p class="image-container"><img alt="img" src="img/b27e33a1fab4b82a.jpg"></p>
<p>Repeat the steps to load the <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> table:</p>
<p>From the flow click <strong>+Dataset</strong></p>
<p class="image-container"><img alt="img" src="img/58a7a9fc6c05b8ff.png"></p>
<p>Point to <strong>SQL databases</strong> then <strong>Snowflake</strong>:</p>
<p class="image-container"><img alt="img" src="img/22f5712d9099e224.png"></p>
<p>Type <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> (ALL UPPERCASE) under Table and <strong>PUBLIC</strong> under Schema:</p>
<p class="image-container"><img alt="img" src="img/1611549c4f5c6b8.jpg"></p>
<p>Click <strong>Test Table</strong>, then click <strong>Create</strong>:</p>
<p class="image-container"><img alt="img" src="img/1983689d488f5e08.jpg"></p>
<p>Go back to flow (keyboard shortcut G+F). Your flow should now look like this:</p>
<p class="image-container"><img alt="img" src="img/fad47ccff8e27333.png"></p>
<p>Double click on the <strong>JHU_COVID_19</strong> dataset</p>
<p class="image-container"><img alt="img" src="img/dbd3f338e864950e.png"></p>
<p>The <strong>JHU_COVID_19</strong> table contains data on a location and day basis about the number and types of cases (Active, Confirmed, Deaths, Recovered) that day.</p>
<p>Dataiku reads a sample of 10000 rows by default. The sampling method can be changed under <strong>Configure Sample</strong> but for this lab we can leave it as the default:</p>
<p class="image-container"><img alt="img" src="img/8db4e16fb1e28401.png"></p>
<p>Dataiku automatically detects data type and meaning of each column. The status bar shows how much of the data is valid (green), invalid (red), and missing (grey). You can view column Stats (data quality, distributions) by clicking <strong>Quick Column Stats</strong> button on the right:</p>
<p class="image-container"><img alt="img" src="img/f0a4fa1ad1d86ea5.jpg"></p>
<p>Click the <strong>close</strong> button when you are finished:</p>
<p class="image-container"><img alt="img" src="img/70b3653d0fe1139c.jpg"></p>
<p>If you would like a deeper view click on a column name, then select <strong>Analyze</strong> to see column-level statistics (of the sample!)</p>
<p class="image-container"><img alt="img" src="img/c6da96ab3cd92398.png"></p>
<p class="image-container"><img alt="img" src="img/33adfc4e65bb0760.png"></p>
<p>Navigate back to the flow. As a reminder you can always do this simply by clicking the <strong>Flow</strong> menu item in the top left corner or by using one of the many keyboard shortcuts G+F.</p>
<p class="image-container"><img alt="img" src="img/cfad307c026e9b63.jpg"></p>
<p>You should now see your two datasets in the flow once more:</p>
<p class="image-container"><img alt="img" src="img/abe2f995ae7d94b8.jpg"></p>
<p>Explore the <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> dataset by double clicking from the flow:</p>
<p class="image-container"><img alt="img" src="img/67c1894c035bf115.png"></p>
<p>The <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> table contains data on a location and day basis about the percent change in movement across several categories (grocery stores and pharmacies, parks, etc.) More information about this dataset is <a href="https://www.google.com/covid19/mobility/" target="_blank">here</a>.</p>
<p>Navigate back to the flow. Either by clicking the <strong>Flow</strong> menu item in the top left corner or by using the keyboard shortcut G+F.</p>
<h2 is-upgraded>Data Preparation</h2>
<p><strong>Group.</strong> We&#39;ll start by performing some simple aggregations on our two datasets. This will help us understand changes in mobility and new cases of COVID-19 across geographic regions. To do that we&#39;ll use the <strong>Group</strong> recipe.</p>
<p>Single click the <strong>JHU_COVID_19</strong> table and then select <strong>Group</strong> under Visual recipes on the panel on the right:</p>
<p class="image-container"><img alt="img" src="img/741c888de6357e5e.jpg"></p>
<p>Select <strong>COUNTRY_REGION</strong> in the <strong>Group By</strong> dropdown, Click <strong>Create Recipe</strong>:</p>
<p class="image-container"><img alt="img" src="img/f0668d74ca127ac7.jpg"></p>
<p>We&#39;ll want to add some additional keys to group on.</p>
<p>In the top section <strong>Group Keys</strong> use the <strong>Select key to add</strong> dropdown followed by the <strong>ADD</strong> button to add grouping keys for <strong>PROVINCE_STATE</strong>, <strong>DATE</strong> and <strong>CASE_TYPE</strong></p>
<p>In the same section uncheck the <strong>Compute count for each group</strong> option in the centre of the screen as we don&#39;t require this stat.</p>
<p>We&#39;d like to see new cases so in the main <strong>Per field aggregations</strong> section locate the field <strong>Difference</strong> add the aggregation <strong>Sum.</strong></p>
<p>Your screen should now look like this:</p>
<p class="image-container"><img alt="img" src="img/ebe45ddb11eeada9.jpg"></p>
<p>There are four case types in the John Hopkins data but we only want to use and predict using Deaths and Confirmed.  So, we will filter out Active and Recovered records.  We will also filter out the problematic Peru data as discovered in the previous section.<br> To do this:</p>
<ul>
<li>Select Pre-Filter on the left of your Group recipe screen.</li>
<li>Toggle the Filter on and specify in the dropdown to Keep only rows that satisfy: ‘all the following conditions&#39;</li>
<li>Select CASE_TYPE in the dropdown, then ‘is different from&#39; then enter Recovered</li>
<li>Click on the + Add A Condition button and select CASE_TYPE again in the dropdown, then  ‘is different from&#39; then enter Active</li>
<li>Click on the + Add A Condition button and select COUNTRY_REGION in the dropdown, then  ‘is different from&#39; then enter Peru</li>
</ul>
<aside class="special"><p>Note: These values are case sensitive so ‘recovered&#39;, ‘active&#39; and ‘PERU&#39; would not work correctly.  Also, in the step above be sure to select ‘all the following conditions&#39; rather than ‘all the following conditions column&#39;</p>
</aside>
<p>Your screen should now look like this:</p>
<p class="image-container"><img alt="img" src="img/4a329110e8dfd7e4.png"></p>
<p>Then click <strong>RUN</strong> on the bottom left to execute the recipe - make sure the compute engine is <strong>In-database (SQL)</strong> to push computation down to Snowflake (click the three cogs to change if required).</p>
<p class="image-container"><img alt="img" src="img/e108c9be6f1cc640.png"></p>
<p>Since we are making changes to the schema we will receive a warning of this, go ahead and click <strong>Update Schema.</strong></p>
<p class="image-container"><img alt="img" src="img/2f51bd244333baea.jpg"></p>
<p>The job will run and if all went well you will receive a <strong>Job Succeeded</strong> message at the foot of the page:</p>
<p class="image-container"><img alt="img" src="img/1c037a214dfe24af.jpg"></p>
<p><strong>Return to the flow</strong> either by <strong>clicking the Flow icon</strong> in the top left or <strong>press G + F on the keyboard:</strong></p>
<p class="image-container"><img alt="img" src="img/14e8cd3ffa4469fc.jpg"></p>
<p>We will now aggregate on the Mobility dataset from Google. From the flow <strong>single click</strong> on <strong>GOOG_GLOBAL_MOBILITY_REPORT</strong> and select <strong>Group</strong> from the Visual Recipes. <strong>Group by COUNTRY_REGION</strong> and click <strong>Create Recipe</strong>.</p>
<p class="image-container"><img alt="img" src="img/20ac56aa9b0ebcfd.jpg"></p>
<p>In the <strong>Group</strong> stage of the recipe add <strong>PROVINCE_STATE</strong> and <strong>DATE</strong> as additional grouping keys in the same way as previously.</p>
<p>Either individually or using the mass <strong>Actions</strong> button check the following:</p>
<ul>
<li><strong>GROCERY_AND_PHARMACY_CHANGE_PERC</strong></li>
<li><strong>PARKS_CHANGE_PERC</strong></li>
<li><strong>RESIDENTIAL_CHANGE_PERC</strong></li>
<li><strong>RETAIL_AND_RECREATION_CHANGE_PERC</strong></li>
<li><strong>TRANSIT_STATIONS_CHANGE_PERC</strong></li>
<li><strong>WORKPLACES_CHANGE_PERC</strong></li>
</ul>
<p>Your screen should now look like this:</p>
<p class="image-container"><img alt="img" src="img/a37fabb9df62f025.jpg"></p>
<p>Click the <strong>Actions</strong> Dropdown and apply <strong>Avg</strong> to your six selected columns. As before <strong>uncheck</strong> the <strong>Compute count for each group</strong> since we don&#39;t need this.</p>
<p class="image-container"><img alt="img" src="img/2df5e66175e0596a.jpg"></p>
<p>Then click <strong>RUN</strong> on the bottom left to execute the recipe - make sure the compute engine is In-database (SQL)</p>
<p class="image-container"><img alt="img" src="img/e108c9be6f1cc640.png"></p>
<p>As before we are making changes to the schema we will receive a warning of this, go ahead and click <strong>Update Schema</strong> and once you have explored your new aggregated dataset.</p>
<p><strong>Press G + F to return to the flows screen</strong>.</p>
<p>The <strong>JHU_COVID_19</strong> table includes a column for case type and the difference summed from the previous row that we grouped by country regions, state, date and type in our earlier step.</p>
<p>However, we&#39;d like to change the data format so that case type and difference are their own separate columns (i.e. Confirmed &amp; Deaths are separate columns). We can do this with a <strong>Pivot</strong> recipe.</p>
<p>From the flow single click the <strong>JHU_COVID_19_by_COUNTRY_REGION</strong> and then select <strong>Pivot</strong>:</p>
<p class="image-container"><img alt="img" src="img/915037ed6cc0984.jpg"></p>
<p>In the dropdown <strong>Pivot By</strong> select <strong>CASE_TYPE</strong>.</p>
<p>Click <strong>CREATE RECIPE</strong>:</p>
<p class="image-container"><img alt="img" src="img/d1626134c3646f61.jpg"></p>
<p>You should see this screen:</p>
<p class="image-container"><img alt="img" src="img/c375a0b725175bca.jpg"></p>
<p>First, let&#39;s use one of the examples from the top left <strong>Examples</strong> box. Select <strong>Pivot Table</strong>.</p>
<p>Then select some Row identifiers. These are columns that will be retrieved as is in the output table. Click &#34;Add new column&#34; under <strong>Row identifiers</strong>.</p>
<p class="image-container"><img alt="img" src="img/9c1a5d46e133bb42.jpg"></p>
<p>Select the following columns: <strong>COUNTRY_REGION</strong>, <strong>PROVINCE_STATE</strong> and <strong>DATE</strong></p>
<p>(Note: The order you add the row identifiers here simply reflects the order the columns will appear in the output dataset. Although not strictly important if you wish your project datasets to mirror the ordering of the ones in this guide stick to the order below)</p>
<p class="image-container"><img alt="img" src="img/21280a064d3989fd.jpg"></p>
<p>Next, the section <strong>Populate content with</strong> will define the values that go under the new columns with <strong>CASE_TYPE</strong>. There are a number of options for aggregations - count of values, min, max, etc.</p>
<p>First, <strong>deselect</strong> the <strong>Count of records</strong>:</p>
<p class="image-container"><img alt="img" src="img/752eecd7470bca5c.png"></p>
<p>Then click on the <strong>Add new</strong> dropdown.</p>
<p>We only have one value here so select <strong>Difference_sum</strong>. The default aggregation is <strong>count</strong> which we do not want. To change this click the <strong>down arrow</strong> to the left of count:</p>
<p class="image-container"><img alt="img" src="img/4611ec7edc0aa625.jpg"></p>
<p>Under <strong>Aggregation</strong> instead of Count, select <strong>Min</strong>.</p>
<p class="image-container"><img alt="img" src="img/3678258fe9acba6e.jpg"></p>
<p>You should now have the following settings. Go ahead and <strong>RUN</strong> the Pivot recipe</p>
<p class="image-container"><img alt="img" src="img/fe002cfadd6d76e4.jpg"></p>
<p>Explore your new dataset to make sure the new columns have properly been created by returning to the flow and double clicking the new dataset.</p>
<p>We can see we have our new columns for types of case (<strong>Confirmed</strong> and <strong>Deaths</strong>) and also see there is quite a lot of missing data for <strong>PROVINCE_STATE</strong>. We&#39;ll address that in the next section.</p>
<p>If you have time it&#39;s also worth <strong>left clicking</strong> on the <strong>Deaths_DIFFERENCE_sum_min</strong> column and selecting <strong>Analyze</strong>. What do you notice about the data distribution? It seems there is a pretty big outlier (run on <strong>Whole data</strong> rather than <strong>Sample</strong> if you do not see it). Again that is something we will want to address in the next section.</p>
<p class="image-container"><img alt="img" src="img/87f46a99f53013fa.jpg"></p>
<p><strong>Return to the flow</strong> and at this stage your project flow should look like this:</p>
<p class="image-container"><img alt="img" src="img/f87e8bbd5f7ae654.jpg"></p>
<p>Whilst the recipes we have used so far such as grouping and pivots perform a given task and are likely already familiar to you, the Prepare recipe is a little different. Think of it like a tool box but instead of having tools for DIY like hammers, chisels and tape measures we have tools for data cleansing, normalization and enrichment. In DSS terminology we call those ‘tools&#39; processors and in the Prepare recipe you use them in a visual and interactive way.</p>
<p>There are a wide range of processors we can use in the Prepare recipe and we are going to use a few of the simplest ones to help us with our missing data.</p>
<p>From the flow <strong>click</strong> on the grouped and pivoted JHU dataset, then select the <strong>Prepare</strong> recipe:</p>
<p class="image-container"><img alt="img" src="img/aa0fe2a28f0634b8.jpg"></p>
<p>We can see that the automatically generated output dataset name is starting to get a little unwieldy after a few transformations.</p>
<p class="image-container"><img alt="img" src="img/f82360401ff69e92.jpg"></p>
<p>Let&#39;s <strong>trim</strong> that to <strong>JHU_COVID_19_prepared</strong> and then click <strong>Create Recipe</strong>.</p>
<p class="image-container"><img alt="img" src="img/1387920ffcb61efe.jpg"></p>
<p>We&#39;d like to handle the missing data in our <strong>PROVINCE_STATE</strong> column, there are many ways the Prepare recipe can help with this and we are going to use probably the simplest technique of all and fill empty rows.</p>
<p><strong>Left click</strong> on the column title <strong>PROVINCE_STATE</strong> then <strong>select More actions</strong> and <strong>Fill empty rows with...</strong></p>
<p class="image-container"><img alt="img" src="img/b3d4121d45cc5964.jpg"></p>
<p>In the break out box simply type <strong>NA</strong> and click <strong>OK</strong>.</p>
<p class="image-container"><img alt="img" src="img/13f16746f8cad716.jpg"></p>
<p>The step gets added to the left. Next we&#39;ll remove rows if <strong>COUNTRY_REGION</strong> or <strong>DATE</strong> is empty.</p>
<p>As well as context sensitive options like we just used you can also access the full library of processors.</p>
<p><strong>Click</strong> on the <strong>+ADD A NEW STEP</strong> button. There are a lot of options here so select <strong>Data Cleansing</strong> from the Processor library and select <strong>Remove rows where cell is empty</strong> processor (you can also use the search box when you may not be sure where a processor lives or wish to explore what options are available for a given task).</p>
<p class="image-container"><img alt="img" src="img/81964feb19576407.jpg"></p>
<p>In the step created on the left select the <strong>multiple</strong> column option and click the <strong>+ADD A COLUMN</strong> button to first add <strong>COUNTRY_REGION</strong> and then add the <strong>DATE</strong> column.</p>
<p class="image-container"><img alt="img" src="img/5023247a0d8ba79a.jpg"></p>
<p>Finally let&#39;s address the outlier we found when we analyzed the <strong>Death_DIFFERENCE_sum_min</strong> column after we pivoted.</p>
<p><strong>Click</strong> the <strong>+ADD A NEW STEP</strong> button again to bring up the processor library. This time <strong>check</strong> the <strong>Filter Data</strong> section and <strong>select</strong> the <strong>Filter rows/cells on value</strong> processor. <strong>Set</strong> the action dropdown to <strong>Remove matching rows</strong>, the column to <strong>DATE</strong>, <strong>click</strong> the blue <strong>+ADD VALUE</strong> option and enter a date of <strong>2020-08-31</strong></p>
<p><strong>NOTE:</strong> We are working on a live dataset that is regularly updated and amended, if errors such as this have been purged from the JHU dataset feel free to skip this step.</p>
<p class="image-container"><img alt="img" src="img/22394b626fc53f01.jpg"></p>
<p class="image-container"><img alt="img" src="img/bf6d300dadcbe23.jpg"></p>
<p>Before running the recipe <strong>check which execution engine</strong> will be used. If it is set to <strong>Local stream</strong> then <strong>click on the three cogs</strong> and <strong>select In-database (SQL)</strong> and click <strong>RUN</strong> to push the computation to Snowflake.</p>
<p class="image-container"><img alt="img" src="img/53cdf42c6405f50.jpg"></p>
<p class="image-container"><img alt="img" src="img/44a59792602e3c5b.jpg"></p>
<p><strong>Press G + F to return to the flows screen</strong></p>
<p>We want to perform the same simple data cleaning steps on our grouped Google Mobility data. Rather than repeat the process simply <strong>single click</strong> on the previously created <strong>prepare recipe</strong> in the flow and select <strong>Copy</strong> from the <strong>Actions menu</strong> on the right.</p>
<p class="image-container"><img alt="img" src="img/f7fbdc02aa09ca3e.jpg"></p>
<p>In the copy break-out box <strong>click the drop down box</strong> under <strong>Replacement for</strong> and find our earlier grouped mobility dataset <strong>GOOG_GLOBAL_MOBILITY_REPORT_by_COUNTRY_REGION</strong> (NOTE: make sure you select this dataset and <strong>not</strong> the original GOOG_GLOBAL_MOBILITY_REPORT)</p>
<p>We need to type an output name, call the dataset <strong>GOOG_prepared</strong> and click <strong>Create Recipe</strong>:</p>
<p class="image-container"><img alt="img" src="img/8e22c447bab4aff6.jpg"></p>
<p>Click <strong>RUN</strong> to execute the copied recipe and select <strong>Update Schema</strong> when prompted.</p>
<p>Now that we&#39;ve applied our transformations, let&#39;s join the <strong>JHU</strong> and <strong>GOOG</strong> tables.</p>
<p><strong>Press G + F to return to the flows screen</strong></p>
<p><strong>Left click</strong> on <strong>JHU_COVID_19_prepared</strong> then <strong>hold down the shift key</strong> and also <strong>left click</strong> on <strong>GOOG_prepeared</strong>. This will update the visual recipes in the right hand panel to only those applicable to multiple datasets. Choose the <strong>Join</strong> recipe.</p>
<p class="image-container"><img alt="img" src="img/ce2b206db65cbb65.jpg"></p>
<p><strong>Click</strong> on <strong>CREATE RECIPE.</strong></p>
<p class="image-container"><img alt="img" src="img/b8c61c136a29b880.jpg"></p>
<p>Dataiku automatically detects potential keys to join on. The default join type is a <strong>Left Join</strong>. We want to change this to an <strong>Inner join</strong> so we only keep matches. To change the join type and conditions, <strong>click</strong> on the <strong>= sign</strong> between one of the keys.</p>
<p class="image-container"><img alt="img" src="img/d36f131e3541f5ac.jpg"></p>
<p><strong>Select Join Type</strong>, then <strong>select Inner Join</strong> and then <strong>Close</strong>.</p>
<p class="image-container"><img alt="img" src="img/90ac95a3fc11ad8c.jpg"></p>
<p>Accept the remaining defaults and click on <strong>RUN</strong>.</p>
<p class="image-container"><img alt="img" src="img/c3a78905a2add235.jpg"></p>
<p>Now we have explored, cleaned, aggregated and joined our data, we now want to create some features. We&#39;d like to compute some lag features to see if past trends can help predict future ones when we start modelling. We can use the <strong>Window</strong> recipe for this (and much more).</p>
<p>A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities.</p>
<p>In other words, unlike the Group recipe we used earlier, a Window recipe does not reduce the number of rows in a dataset. It creates new columns in a dataset that are the results of computations that use all rows in a &#34;window&#34;, that is, a subset, of all rows of the dataset.</p>
<p><strong>Press G + F to return to the flows screen</strong></p>
<p><strong>Click</strong> on the <strong>JHU_COVID_19_prepared_joined</strong> dataset, then the <strong>Window</strong> recipe. In the <strong>New window recipe</strong> screen accept the defaults and <strong>click CREATE RECIPE</strong>:</p>
<p class="image-container"><img alt="img" src="img/cb2d5d1725dbaebd.jpg"></p>
<p class="image-container"><img alt="img" src="img/23a468f88b3a9a29.jpg"></p>
<p>Under <strong>Window definitions</strong>, <strong>toggle on Partitioning Columns</strong> then <strong>select COUNTRY_REGION</strong> and <strong>PROVINCE_STATE</strong>. As we are creating historical lag variables lets go ahead and <strong>toggle on Order Columns</strong> and select <strong>DATE</strong>:</p>
<p class="image-container"><img alt="img" src="img/c34be93ff14bd999.jpg"></p>
<p><strong>Click</strong> on the <strong>Aggregations</strong> step on the left and <strong>select all columns except the ones we used for partitioning and ordering (COUNTRY_REGION, PROVINCE_STATE and DATE)</strong>. Rather than individually select each aggregation <strong>click the down arrow</strong> on the <strong>Action</strong> button to perform a mass action.</p>
<p class="image-container"><img alt="img" src="img/e29b3e74862c3092.jpg"></p>
<p><strong>Retrieve</strong> is already selected, check <strong>Value in a previous row</strong> and <strong>enter 1,2,7,14 in each lag box</strong> (this last step will need to be done individually). This gives us the previous day&#39;s value, the day before, one week and two weeks ago.</p>
<p class="image-container"><img alt="img" src="img/2bb17759205cc1de.jpg"></p>
<p><strong>RUN</strong> the recipe (make sure the engine is <strong>In-database</strong> and <strong>Update Schema</strong>). Also note all the new columns (features) this recipe is creating in the schema changes dialog box.</p>
<p class="image-container"><img alt="img" src="img/e108c9be6f1cc640.png"></p>
<p class="image-container"><img alt="img" src="img/6731aed896644bc9.jpg"></p>
<p><strong>Press G + F to return to the flows screen</strong></p>
<p><strong>Flow Check:</strong> Your flow should look like this:</p>
<p class="image-container"><img alt="img" src="img/d21fe3e5e1cf8676.jpg"></p>
<h2 is-upgraded>Optional Data Preparation Steps</h2>
<p>There are plenty of additional data prep steps that we do not have time to cover today. For example:</p>
<ol type="1">
<li>Normalize numerical features (either in a visual Prepare recipe or SQL recipe)</li>
<li>Create additional features in the Window recipe</li>
<li>Many more - feel free to explore outside of this lab!</li>
</ol>
<h2 is-upgraded>Machine Learning</h2>
<p>Before applying a machine learning model, we will split the data by:</p>
<ol type="1">
<li>80% of data → train</li>
<li>20% of data → test</li>
</ol>
<p><strong>Select the joined dataset</strong> and then the <strong>Split</strong> recipe:</p>
<p class="image-container"><img alt="img" src="img/26c4cbd02892ff0e.jpg"></p>
<p><strong>Click ADD</strong> to specify the two datasets (train and test):</p>
<p class="image-container"><img alt="img" src="img/f8bf0f33b3bfb1a4.jpg"></p>
<p>Name one of the datasets <strong>train</strong> and then click <strong>CREATE DATASET</strong>:</p>
<p class="image-container"><img alt="img" src="img/525616179ff40a8e.jpg"></p>
<p>Repeat for test - click <strong>ADD</strong>, then name dataset <strong>test</strong>, then click <strong>CREATE DATASET</strong>.</p>
<p class="image-container"><img alt="img" src="img/e89b3bac6a4a163.jpg"></p>
<p>Once you&#39;ve created both datasets, click <strong>CREATE RECIPE:</strong></p>
<p>For splitting method, <strong>select Dispatch percentiles of sorted data</strong>:</p>
<p class="image-container"><img alt="img" src="img/7c0b451fa1d445bb.jpg"></p>
<p><strong>Sort by date</strong> and specify a standard 80/20 split for train/test:</p>
<p class="image-container"><img alt="img" src="img/68d7a16f91816be2.jpg"></p>
<p>Finally select the <strong>Pre-Filter</strong> stage. We only want to keep rows where the death difference is greater than zero so <strong>toggle on Filter</strong>, then <strong>select Deaths_DIFFERENCE_sum_min</strong> and specify is greater than <strong>(&gt;) 0</strong>.</p>
<p class="image-container"><img alt="img" src="img/e6286264101e29c9.jpg"></p>
<p><strong>RUN</strong> the Split recipe:</p>
<p class="image-container"><img alt="img" src="img/e108c9be6f1cc640.png"></p>
<p>Now we will start our modeling. As a reminder we are looking to use our historical data and the features we created in the flow to try to predict changes in death rates.</p>
<p><strong>Press G + F to return to the flows screen</strong></p>
<p>From the flow <strong>single click</strong> the <strong>train set</strong> and then click <strong>LAB</strong>:</p>
<p class="image-container"><img alt="img" src="img/3cd210f970c830fe.png"></p>
<p>Select AutoML Prediction:</p>
<p class="image-container"><img alt="img" src="img/c9a7a835de6640d4.jpg"></p>
<p><strong>Select</strong> our target variable <strong>Deaths_DIFFERENCE_sum_min</strong>, then select <strong>Quick Prototypes</strong> and <strong>Create</strong>:</p>
<p class="image-container"><img alt="img" src="img/febb13b0d423ecca.jpg"></p>
<p><strong>Select</strong> the <strong>DESIGN</strong> tab to configure your model training parameters. Under the <strong>Train/Test Set</strong> section on the left of the screen, <strong>enable TIME ORDERING</strong> and set <strong>DATE</strong> as the time variable. Since we are not working with a huge dataset we can go ahead and set the <strong>Sampling Method</strong> to <strong>No Sampling (whole data)</strong>:</p>
<p class="image-container"><img alt="img" src="img/54e26fc885a14245.jpg"></p>
<p>Next select the <strong>Features Handling</strong> section. This is where we can toggle features (should they be used during machine learning? How should variable type be handled if it is used?). For our prediction we will work with just numerical data so let&#39;s <strong>toggle off COUNTRY_REGION, PROVINCE_STATE and DATE</strong>. The remaining numerical features can remain as set allowing for rescaling prior to training.</p>
<p class="image-container"><img alt="img" src="img/e22ce8f71a25cf50.jpg"></p>
<p>Finally select the <strong>Algorithm</strong> section. We have many options here but let&#39;s concentrate on some simple regression and <strong>toggle on</strong> only <strong>Ridge Regression and Lasso Regression</strong>. Once done, <strong>click</strong> on the <strong>Train</strong> button (top right) to train the models.</p>
<p class="image-container"><img alt="img" src="img/af663b1beb117e4d.jpg"></p>
<p>You can optionally name your training session and give a description or just leave blank. <strong>Click Train</strong> again.</p>
<p class="image-container"><img alt="img" src="img/32afd06443e298de.jpg"></p>
<p>DSS will spin up resources and train the model(s) in line with our design settings. For the first session we can see that the Lasso regression outperformed the Ridge regression in regards to the metric we&#39;re evaluating for (R2 score in this case where the closer to 1 the better).</p>
<p><strong>Drilldown</strong> into the details of the best performing model either by <strong>clicking</strong> on its name in the <strong>session window</strong> or the <strong>results window:</strong></p>
<p class="image-container"><img alt="img" src="img/f7b056f4b07dd646.jpg"></p>
<p>We have a number of tools and visualizations here to help us both interpret the model and better understand its performance. Let&#39;s take a look at the coefficients by <strong>clicking</strong> on the <strong>Regression coefficients</strong> option on the left under the <strong>INTERPRETATION</strong> menu, perhaps unsurprisingly recent deaths are important but so too are residential changes for the training session run.</p>
<p><strong>Note:</strong> When in the model summary screen, if you ever wish to return to your model training and design screen just <strong>click</strong> on <strong>Models</strong> at the top.</p>
<p class="image-container"><img alt="img" src="img/f1d50189571f8178.jpg"></p>
<p>Under the <strong>Performance</strong> section on the left hand menu <strong>click</strong> on <strong>Scatter Plot</strong>:</p>
<p>If the model was perfect all the points would be on the diagonal line. This would mean that the predicted values are exactly equal to the actual values. The points below the line are underestimates, above the line are overestimates. Your general aim is to minimize the distance from the points to the diagonal.</p>
<p>This example could be improved on, but is reasonable for a first run:</p>
<p class="image-container"><img alt="img" src="img/469e1ecec0e5c468.jpg"></p>
<p>Let&#39;s also <strong>click</strong> on <strong>Subpopulation analysis</strong> which is under the <strong>Interpretation</strong> menu. This can be useful for assessing if our model behaves the same across different subpopulations. In our case let&#39;s analyse by country. <strong>Select COUTRY_REGION</strong> from the <strong>dropdown</strong> and click <strong>COMPUTE</strong>. In my example we can see that while the model performed well on data for the US, Mexico, and India it did not do so well in many other countries.</p>
<p class="image-container"><img alt="img" src="img/916ff9af439fb72c.jpg"></p>
<p>This is something we would certainly want to explore further and we would now begin an iterative process of experimentation/refinement to understand and improve our model so feel free to work on your model (and data flow) after the lab but for now lets go ahead and deploy our model.</p>
<p><strong>Click</strong> on <strong>DEPLOY</strong> button on the top right and then accept the defaults and <strong>click CREATE</strong>:</p>
<p class="image-container"><img alt="img" src="img/9a5bb07982dadbd9.jpg"></p>
<p>Now our model is deployed to our flow:</p>
<p class="image-container"><img alt="img" src="img/15b20529c0781c58.jpg"></p>
<p>For our final task we will score our deployed model against our test dataset to evaluate model fit. <strong>Single click</strong> on the <strong>model</strong> in the Flow and then <strong>select Score</strong> from the <strong>actions menu</strong>:</p>
<p class="image-container"><img alt="img" src="img/e9761f34563532bc.jpg"></p>
<p>Apply our <strong>test dataset</strong> as the <strong>Input dataset</strong> and <strong>click Create Recipe</strong>:</p>
<p class="image-container"><img alt="img" src="img/71f959190cecf1eb.jpg"></p>
<p>Finally ensure your execution engine is set to <strong>In-Database (SQL) and Run</strong>.</p>
<p>Congratulations! You have built and scored your model.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Reviewing Dataiku Predictions in Snowflake" duration="5">
        <p>Now we&#39;re going to look at the Snowflake table where we&#39;ve written the predictions.</p>
<p>Switch back to the Snowsight Tab and set the context by running:</p>
<pre><code language="language-sql" class="language-sql">use role sysadmin;
use warehouse compute_wh;
use database pc_dataiku_db;
</code></pre>
<p>Now run the following SQL to see the scored data from the Dataiku model written back to Snowflake. Your table name will be something like this:</p>
<pre><code language="language-sql" class="language-sql">show tables like &#39;%scored%&#39;;
select * from &#34;COVID19_TEST_SCORED_space-3a316aed-dku_node-df3ee930&#34;;
</code></pre>
<p>Use the name returned from the ‘show tables&#39; command and paste in the select statement.</p>
<p>Now we can view the data on a chart from our results:</p>
<pre><code language="language-sql" class="language-sql">Add Column and select ‘Country_region&#39;
</code></pre>
<p class="image-container"><img alt="img" src="img/58bd0454fdfd1927.png"></p>
<h2 is-upgraded>Create an Environment for the Testing Team via Cloning</h2>
<p>Snowflake allows you to create clones, also known as &#34;zero-copy clones&#34; of tables, schemas, and databases in seconds. A snapshot of data present in the source object is taken when the clone is created, and is made available to the cloned object. The cloned object is writable, and is independent of the clone source. That is, changes made to either the source object or the clone object are not part of the other.</p>
<p>A popular use case for zero-copy cloning is to clone a production environment for Development &amp; Testing to do testing and experimentation without (1) adversely impacting the production environment and (2) eliminating the need to set up and manage two separate environments for production and Development &amp; Testing.</p>
<aside class="special"><p><strong>Zero-Copy Cloning FTW! <br></strong> A massive benefit is that the underlying data is not copied; just the metadata/pointers to the underlying data change. Hence &#34;zero-copy&#34; and storage requirements are not doubled when data is cloned. Most data warehouses cannot do this; for Snowflake it is easy!</p>
</aside>
<p>Let&#39;s return to the old UI and select the worksheet Covid19.</p>
<p class="image-container"><img alt="img" src="img/ce2cef5515bfaf31.png"></p>
<p>Run the following command in the worksheet to create a development (dev) database.</p>
<pre><code language="language-sql" class="language-sql">use role sysadmin;
use warehouse compute_wh;
    use database pc_dataiku_db;
    use schema public;
    create database dataiku_test_db clone pc_dataiku_db;
</code></pre>
<p>If closed, expand the database objects browser on the left of the worksheet.  Click the small Refresh button in the left-hand panel and expand the object tree under the DATAIKU_TEST_DB database.</p>
<p>Check that you can see a new table under the DATAIKU_TEST_DB database.  The development team now can do whatever they want with these tables, including even deleting without having any impact on the production tables or any other objects.</p>
<p class="image-container"><img alt="img" src="img/cdbb79e5e8e57956.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Using Time Travel" duration="5">
        <p>Snowflake&#39;s Time Travel capability enables accessing historical data at any point within a pre-configurable period of time. The default period of time is 24 hours and with Snowflake Enterprise Edition it can be up to 90 days. Most data warehouses cannot offer this functionality; with Snowflake it is easy!</p>
<p>Some useful applications of this include:</p>
<ul>
<li>Restoring data-related objects (tables, schemas, and databases) that may have been accidentally or intentionally deleted or updated</li>
<li>Duplicating and backing up data from key points in the past</li>
<li>Analyzing data usage/manipulation over specified periods of time</li>
</ul>
<p>We will run through the following scenarios. 1: A user thinks that all testing is complete and decides to drop a temporary table. Unfortunately they drop the wrong table! 2: A user updates a table but forgets the where clause. The change is made to all rows in the table! 3: Create a clone of a table at a point in time 4: Roll back all changes made to a test table to refresh the table for testing</p>
<h2 is-upgraded>Drop and Undrop a Table</h2>
<p>First let&#39;s see how we can restore data objects that have been accidentally or intentionally deleted.</p>
<p>Let&#39;s set the proper context:</p>
<pre><code language="language-sql" class="language-sql">use database dataiku_test_db;    use schema public;
</code></pre>
<p>Let&#39;s find the name of the table we are going to work with:</p>
<pre><code language="language-sql" class="language-sql">show tables like &#39;%scored%&#39;;
</code></pre>
<p>From the results, you can expand the name column and copy the table name that will look something like ‘COVID19_TRAIN_space-3a316aed-dku_node-df3ee930&#39;</p>
<p>To make life easier, we will rename this table. Run the following SQL:</p>
<pre><code language="language-sql" class="language-sql">Alter table &#34;&lt;table name from above&gt;&#34; rename to di_train;Example : alter table &#34;COVID19_TEST_SCORED_space-3a316aed-dku_node-df3ee930&#34; rename to covid19_test_scored;
</code></pre>
<p>From the worksheet, run the following command which will drop (remove) the covid19_train table:</p>
<pre><code language="language-sql" class="language-sql">drop table covid19_test_scored;
</code></pre>
<p>Now run a SELECT statement on the covid19_train table. In the &#34;<strong>Results</strong>&#34; pane you should see an error because the underlying table has been dropped.</p>
<pre><code language="language-sql" class="language-sql">select * from covid19_test_scored limit 10;
</code></pre>
<p class="image-container"><img alt="img" src="img/4a92484fb952060a.png"></p>
<p>Now restore the table:</p>
<pre><code language="language-sql" class="language-sql">undrop table covid19_test_scored;
</code></pre>
<p>The covid19_test_scored table should be restored.</p>
<p class="image-container"><img alt="img" src="img/5b9db86a0d8e262c.png"></p>
<p>Now run a SELECT statement on the covid19_train table. In the &#34;<strong>Results</strong>&#34; pane you should see data because the underlying table has been restored.</p>
<pre><code>select * from covid19_test_scored limit 10;
</code></pre>
<h2 is-upgraded>Roll Back a Table</h2>
<p>Now let&#39;s look at rolling back a table to a previous state to fix an unintentional DML error that replaces all the province_state names in the covid19_test_scored table with the word &#34;oops.&#34;</p>
<p>Run a query that returns the top 20 Locations - this is what it should look like before update:</p>
<pre><code language="language-sql" class="language-sql">select province_state as &#34;Location&#34;, count(*) as &#34;count&#34;from covid19_test_scoredgroup by 1order by 2 desclimit 20;
</code></pre>
<p>Run the following command that replaces all of the province names in the table with the word &#34;oops&#34;.</p>
<pre><code language="language-sql" class="language-sql">update covid19_test_scored set province_state = &#39;oops&#39;;
</code></pre>
<p>Now run a query that returns the top 20 Locations - notice how we&#39;ve screwed up the station names so we only get one row:</p>
<pre><code language="language-sql" class="language-sql">select province_state as &#34;Location&#34;, count(*) as &#34;count&#34;from covid19_test_scoredgroup by 1order by 2 desclimit 20;
</code></pre>
<p class="image-container"><img alt="img" src="img/cfcbf19b2f1e6384.png"></p>
<p>Normally, we would need to scramble and hope we have a backup lying around. But in Snowflake, we can simply review the history to find transactions that ran against the database. Let&#39;s change the role in the main UI window to sysadmin:</p>
<p class="image-container"><img alt="img" src="img/8bac62dc1d184c9c.png"></p>
<p>Click on the History tab:</p>
<p class="image-container"><img alt="img" src="img/38e953ea20151d69.png"></p>
<p>Here you can see all of the commands that you have run against Snowflake. Let&#39;s find the command that caused the issue. If you see a filter go to 7.2.7.</p>
<p class="image-container"><img alt="img" src="img/b21cd396e51cd9a3.png"></p>
<p>If you see no filter, please click ‘Add a filter&#39;</p>
<p class="image-container"><img alt="img" src="img/d8dd36ee70dd5112.png"></p>
<p>Change filter to ‘SQL Text&#39; and ‘oops&#39; like :</p>
<p class="image-container"><img alt="img" src="img/829c63e53e43cb81.png"></p>
<p>Click on the Query ID for the SQL that ran the incorrect update statement. Highlight the Query Id and copy to your clipboard:</p>
<p class="image-container"><img alt="img" src="img/779eef2e83356a48.png"></p>
<p>Before we make any changes, we can check to see if this transaction was the one that caused the issue. Run the following SQL replacing the Query Id that you copied earlier:</p>
<pre><code language="language-sql" class="language-sql">select province_state as &#34;Location&#34;, count(*) as &#34;count&#34; from covid19_test_scored before(statement =&gt; &#39;01982883-0042-3ced-0000-01f1000463fe&#39;) group by 1 order by 2 desc limit 20;
</code></pre>
<p>Now that we have validated that this is the correct SQL, let&#39;s be safe and create a clone with the original values first, using the following SQL, again using the Query ID from earlier:</p>
<pre><code language="language-sql" class="language-sql">create or replace table covid19_test_scored_rewind clone covid19_test_scored before(statement =&gt; &#39;01982883-0042-3ced-0000-01f1000463fe&#39;);
</code></pre>
<p>Run the SELECT statement again to check that the province_state names have been restored in the newly cloned table:</p>
<pre><code language="language-sql" class="language-sql">select province_state  as &#34;Location&#34;, count(*) as &#34;Count&#34;from covid19_test_scored_rewindgroup by 1order by 2 desclimit 20;
</code></pre>
<p class="image-container"><img alt="img" src="img/d7a728a801c02e02.png"></p>
<p>Now that we&#39;re happy the clone has the correct values, we will use another create feature in Snowflake and swap the 2 tables over. We will rename the newly created clone to the main table name and rename the broken table to the clone name.  Use the following SQL to Swap names:</p>
<pre><code language="language-sql" class="language-sql">alter table covid19_test_scored_rewind swap with covid19_test_scored;
</code></pre>
<p>Run the SELECT statement again to check that the province_state names have been restored in the newly swapped table:</p>
<pre><code language="language-sql" class="language-sql">select province_state  as &#34;Location&#34;, count(*) as &#34;Count&#34;from covid19_test_scoredgroup by 1order by 2 desclimit 20;
</code></pre>
<p class="image-container"><img alt="img" src="img/d7a728a801c02e02.png"></p>
<p>Congratulations, you have now completed this lab! Let&#39;s wrap things up in the next, and final, section.</p>


      </google-codelab-step>
    
      <google-codelab-step label="(Optional) Resetting Your Snowflake Environment" duration="0">
        <p>Lastly, if you would like to reset your environment by deleting all the objects created as part of this lab, run the SQL below in a worksheet.</p>
<p>Run this SQL to set the worksheet context:</p>
<pre><code language="language-sql" class="language-sql">use role accountadmin;
use warehouse compute_wh;
use database covid19;
use schema public;
</code></pre>
<p>Then run this SQL to drop all the objects we created in the lab and resize the warehouse to XSmall:</p>
<pre><code language="language-sql" class="language-sql">drop database if exists dataiku_test_db;
alter warehouse &#34;PC_DATAIKU_WH&#34; set warehouse_size = ‘XSMALL&#39;;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion &amp; Next Steps" duration="1">
        <p>This tutorial was designed as a hands-on introduction to Snowflake and Dataiku to simultaneously teach you how to use it, while showcasing some of its key capabilities.</p>
<p>We encourage you to continue with your free trial by loading in your own sample or production data and by using some of the more advanced capabilities of Snowflake not covered in this lab. There are several ways Snowflake can help you with this:</p>
<ul>
<li>At the very top of the UI click on the &#34;Partner Connect&#34; icon to get access to trial/free ETL and BI tools to help you get more data into Snowflake and then analyze it</li>
<li>Read the &#34;[Definitive Guide to Maximizing Your Free Trial](https://www.snowflake.com/test-driving-snowflake-the-definitive-guide-to-maximizing-your-free-trial/?utm_source=Snowflake&amp;utm_medium=lab guide&amp;utm_campaign=dataiku-vhol-download-maximizing-your-free-trial-guide)&#34;</li>
<li>Attend a Snowflake virtual or in-person <a href="https://www.snowflake.com/about/webinars/" target="_blank">event</a> to learn more about our capabilities and how customers use us</li>
<li>Contact [Sales](https://www.snowflake.com/free-trial-contact-sales/?utm_source=Snowflake&amp;utm_medium=lab guide&amp;utm_campaign=dataiku-vhol-contact-sales-from-lab-guide) to learn more</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
